---
title: Configure LangSmith Deployment for scale
sidebarTitle: Deployment
---

The default configuration for self-hosted LangSmith Deployment can handle substantial read and write load, and you can configure your instance to achieve even higher scale as necessary. This page describes scaling considerations and provides examples to help configure your deployment.

For example configurations, refer to [Example Deployment configurations for scale](#example-deployment-configurations-for-scale).

## Summary

The table below provides an overview comparing different LangSmith Deployment configurations for various load patterns (read requests per second / write requests per second) and standard assistant characteristics (average run execution time of 1 seconds, moderate CPU and memory usage):

|  | **[Low / low](#low-reads-low-writes)** | **[Low / high](#low-reads-high-writes)** | **[High / low](#high-reads-low-writes)** | [Medium / medium](#medium-reads-medium-writes) | [High / high](#high-reads-high-writes) |
| :--- | :--- | :--- | :--- | :--- | :--- |
| <Tooltip tip="Number of write requests being processed by the Deployment per second">Write requests per second</Tooltip> | 5 | 5 | 500 | 50 | 500 |
| <Tooltip tip="Number of read requests being processed by the Deployment per second">Read requests per second</Tooltip> | 5 | 500 | 5 | 50 | 500 |
| **API replicas**<br />(1 CPU, 2Gi per replica) | 1 (default) | 6 | 10 | 3 | 15 |
| **Queue replicas**<br />(1 CPU, 2Gi per replica) | 1 (default) | 10 | 1 (default) | 5 | 10 |
| **N_JOBS_PER_WORKER** | 10 (default) | 50 | 10 | 10 | 50 |
| **Redis resources** | 2 Gi (default) | 2 Gi (default) | 2 Gi (default) | 2 Gi (default) | 2 Gi (default) |
| **Postgres resources** | 2 CPU<br />8 Gi (default) | 4 CPU<br />16 Gi memory | 4 CPU<br />16 Gi per replica | 4 CPU<br />16 Gi memory | 8 CPU<br />32 Gi memory |

Below we go into more details about the request handling and scaling patterns as well as provide configuration examples for your LangSmith Deployment.

## Scaling for write load

Write load is primarily driven by the following factors:

- Creation of new runs
- Creation of new checkpoints during run execution
- Writing to long term memory
- Creation of new threads
- Creation of new assistants
- Execution of periodic cron jobs

The following systems are primarily responsible for handling write load:

- API server: Handles initial request and persistence of data to the database.
- Queue: Handles the execution of runs.
- Redis: Handles the storage of ephemeral data about on-going runs.
- Postgres: Handles the storage of all data, including run, thread, assistant, cron job, checkpointing and long term memory

### Best practices for scaling the write path

#### Enable the use of queue replicas

By default, the api replica manages the queue and does not use queue replicas. You can enable the use of queue replicas by setting the `queue.enabled` configuration to `true`.

```yaml
queue:
  enabled: true
```

This will allow the api replica to offload the queue management to the queue replicas, significantly reducing the load on the api replica and allowing it to focus on handling requests.

#### Change N_JOBS_PER_WORKER based on assistant characteristics

The default value of N_JOBS_PER_WORKER is 10. You can change this value to scale the maximum number of runs that can be executed at a time by a single queue replica based on the characteristics of your assistant.

Some general guidelines for changing N_JOBS_PER_WORKER:

- If your assistant is computationally intensive, lower N_JOBS_PER_WORKER to handle fewer concurrent runs per replica or keep it at the default.
- If your assistant is computationally lightweight, increase N_JOBS_PER_WORKER to handle more concurrent runs per replica.

There is no upper limit to N_JOBS_PER_WORKER. However, the polling algorithm queues use to pick up new runs is greedy, meaning they will try to pick up as many runs as possible and begin executing them right away. If N_JOBS_PER_WORKER is too high and traffic is burstly, you may see uneven replica utilization and potentially longer run execution times.

#### Support a number of workers equal to expected throughput

The more runs you execute in parallel, the more workers you will need to handle the load. There are two main parameters to scale the available workers:

- number_of_queue_replicas: The number of queue replicas provisioned.
- N_JOBS_PER_WORKER: The number of runs that can be executed at a time by a single queue replica. Defaults to 10.

You can calculate the available workers with the following equation:
```
available_workers = number_of_queue_replicas * N_JOBS_PER_WORKER
```

Throughput is then the number of runs that can be executed per second by the available workers:
```
throughput_per_second = available_workers / average_run_execution_time_seconds
```

So the minimum number of workers you should provision to support your expected steady state throughput is:
```
number_of_queue_replicas = throughput_per_second * average_run_execution_time_seconds / N_JOBS_PER_WORKER
```

#### Configure autoscaling for bursty workloads

Autoscaling is disabled by default, but should be configured for bursty workloads. Using the same calculations as above, you can determine the maximum number of workers you should allow the autoscaler to scale to based on maximum expected throughput.

#### Avoid synchronous blocking operations

Avoid synchronous blocking operations in your code. If required, set BG_JOB_ISOLATED_LOOPS to True to run them in a separate event loop and avoid blocking the main event loop.

#### Minimize redundant checkpointing

Minimize redundant checkpointing by setting `checkpoint_during` to the minimum value necessary to ensure your data is durable.

## Scaling for read load

Read load is primarily driven by the following factors:

- Getting the results of a run
- Getting the state of a thread
- Searching for runs, threads, cron jobs and assistants
- Retrieving checkpoints and long term memory

The following systems are primarily responsible for handling read load:

- API server: Handles the request and direct retrieval of data from the database.
- Database: Handles the storage of all data, including run, thread, assistant, cron job, checkpointing and long term memory.
- Redis: Handles the storage of ephemeral data about on-going runs, including streaming messages from queue replicas to api replicas.

### Best practices for scaling the read path

#### Use pagination and filtering to reduce the number of resources returned per request

If you are searching for resources, use pagination and filtering to reduce the number of resources returned per request.

#### Set a TTL on threads and checkpoints to automatically delete old data

Set a TTL on threads and checkpoints to automatically delete old data.

#### Use resumable streams for extra messaging resilience

If you are particularly sensitive to message loss, you can use resumable streams to ensure that messages are not lost if the connection is interrupted.

By default, streaming uses redis pubsub to send messages from queue replicas to api replicas. This has the advantage of being lightweight and efficient, but does mean that in particularly unstable network conditions, messages may be lost. Enabling resumable streams stores messages in Redis and allows for replay of those messages when the subscriber reconnects.

The added resilience comes at the cost of increased latency and Redis memory usage. It is recommended to use resumable streams only when necessary and can be configured on a per-thread basis.

## Example Deployment configurations

Below we provide some example LangSmith Deployment configurations based on expected read and write loads.

We define each load level as follows:

- Low means roughly 5 requests per second
- Medium means roughly 50 requests per second
- High means roughly 500 requests per second

<Note>
The exact optimal configuration depends on your application complexity, request patterns, and data requirements. Use the examples below in combination with the information above and your specific usage to update your deployment configuration as needed. If you have any questions, please reach out to the LangChain team.
</Note>

### Low reads, low writes <a name="low-reads-low-writes"></a>

The default LangSmith Deployment configuration will handle this load. No custom resource configuration is needed here.

### Low reads, high writes <a name="low-reads-high-writes"></a>

You have a high volume of write requests (500 per second) being processed by your deployment, but relatively few read requests (5 per second).

For this, we recommend a configuration like this:

```yaml
# Example configuration for low reads, high writes (5 read/500 write requests per second)
api:
  replicas: 6
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

queue:
  replicas: 10
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

config:
  numberOfJobsPerWorker: 50

redis:
  resources:
    requests:
      memory: "2Gi"
    limits:
      memory: "2Gi"

postgres:
  resources:
    requests:
      cpu: "4"
      memory: "16Gi"
    limits:
      cpu: "8"
      memory: "32Gi"
```

### High reads, low writes <a name="high-reads-low-writes"></a>

You have a high volume of read requests (500 per second) but relatively few write requests (5 per second).

For this, we recommend a configuration like this:

```yaml
# Example configuration for high reads, low writes (500 read/5 write requests per second)
api:
  replicas: 10
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

queue:
  replicas: 1  # Default, minimal write load
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

redis:
  resources:
    requests:
      memory: "2Gi"
    limits:
      memory: "2Gi"

postgres:
  resources:
    requests:
      cpu: "4"
      memory: "16Gi"
    limits:
      cpu: "8"
      memory: "32Gi"
  # Consider read replicas for high read scenarios
  readReplicas: 2
```

### Medium reads, medium writes <a name="medium-reads-medium-writes"></a>

This is a balanced configuration that should handle moderate read and write loads (50 read/50 write requests per second).

For this, we recommend a configuration like this:

```yaml
# Example configuration for medium reads, medium writes (50 read/50 write requests per second)
api:
  replicas: 3
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

queue:
  replicas: 5
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

redis:
  resources:
    requests:
      memory: "2Gi"
    limits:
      memory: "2Gi"

postgres:
  resources:
    requests:
      cpu: "4"
      memory: "16Gi"
    limits:
      cpu: "8"
      memory: "32Gi"
```

### High reads, high writes <a name="high-reads-high-writes"></a>

You have high volumes of both read and write requests (500 read/500 write requests per second).

For this, we recommend a configuration like this:

```yaml
# Example configuration for high reads, high writes (500 read/500 write requests per second)
api:
  replicas: 15
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"
  autoscaling:
    enabled: true
    minReplicas: 15
    maxReplicas: 25
    targetCPUUtilizationPercentage: 70

queue:
  replicas: 10
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"
  autoscaling:
    enabled: true
    minReplicas: 10
    maxReplicas: 20
    targetCPUUtilizationPercentage: 70

config:
  numberOfJobsPerWorker: 50

redis:
  resources:
    requests:
      memory: "2Gi"
    limits:
      memory: "2Gi"

postgres:
  resources:
    requests:
      cpu: "8"
      memory: "32Gi"
    limits:
      cpu: "16"
      memory: "64Gi"
```

<Note>
Ensure that your deployment environment has sufficient resources to scale to the recommended size. Monitor your applications and infrastructure to ensure optimal performance. Consider implementing monitoring and alerting to track resource usage and application performance.
</Note>
